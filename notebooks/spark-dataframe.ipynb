{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "232007bf-81f9-44f4-9ba2-36ed2c148f36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "employee_data = [\n",
    "    (1, \"Alice\", \"IT\", 60000, \"2021-01-10\", \"India\"),\n",
    "    (2, \"Bob\", \"HR\", 45000, \"2020-03-15\", None),\n",
    "    (3, \"Charlie\", \"IT\", 80000, \"2019-07-23\", \"USA\"),\n",
    "    (4, \"David\", \"Finance\", 55000, \"2021-11-01\", \"India\"),\n",
    "    (5, \"Eva\", \"HR\", None, \"2022-02-19\", \"UK\"),\n",
    "    (6, \"Frank\", \"IT\", 60000, \"2021-01-10\", \"India\"),\n",
    "    (6, \"Frank\", \"IT\", 60000, \"2021-01-10\", \"India\")\n",
    "]\n",
    "\n",
    "emp_cols = [\"emp_id\", \"name\", \"dept\", \"salary\", \"join_date\", \"country\"]\n",
    "\n",
    "\n",
    "emp_df = spark.createDataFrame(employee_data, emp_cols)\n",
    "emp_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70ee1f22-d995-4f80-a615-f6d1a6c8b011",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dept_data = [\n",
    "    (\"IT\", \"Technology\"),\n",
    "    (\"HR\", \"Human Resources\"),\n",
    "    (\"Finance\", \"Finance & Accounting\")\n",
    "]\n",
    "\n",
    "dept_cols = [\"dept\", \"dept_name\"]\n",
    "\n",
    "dept_df = spark.createDataFrame(dept_data, dept_cols)\n",
    "dept_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bb6ece9-74ce-4347-a071-7ccf418188ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "emp_df = emp_df.dropDuplicates([\"emp_id\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35cf129f-c862-4a8c-9038-f6905bfc3f46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "emp_df = emp_df.withColumn(\n",
    "    \"salary\",\n",
    "    when(col(\"salary\").isNull(), 0).otherwise(col(\"salary\"))\n",
    ").withColumn(\n",
    "    \"country\",\n",
    "    when(col(\"country\").isNull(), \"Unknown\").otherwise(col(\"country\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30d2a960-91d1-4ee7-8fd4-0ae7b9f4b483",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "emp_df = emp_df.filter(col(\"salary\") > 50000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0f135e0-3580-45c4-a237-094483f715b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "emp_df = emp_df.withColumn(\n",
    "    \"join_date\",\n",
    "    to_date(col(\"join_date\"), \"yyyy-MM-dd\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcbebaea-6384-496a-a189-d885563179fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, count\n",
    "\n",
    "dept_stats = emp_df.groupBy(\"dept\").agg(\n",
    "    avg(\"salary\").alias(\"avg_salary\"),\n",
    "    count(\"emp_id\").alias(\"emp_count\")\n",
    ")\n",
    "\n",
    "dept_stats.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e3b5d7b-0a2a-4428-87db-f2fb78713f18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "emp_df = emp_df.orderBy(col(\"salary\").desc())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7e92f41-9c1a-4b03-88b7-75659aaa7dba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_df = emp_df.join(dept_df, on=\"dept\", how=\"left\")\n",
    "final_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e38c5f1f-e11a-4e2b-b0c3-83b0bb5e2aea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank\n",
    "\n",
    "window_spec = Window.partitionBy(\"dept\").orderBy(col(\"salary\").desc())\n",
    "\n",
    "final_df = final_df.withColumn(\n",
    "    \"dept_salary_rank\",\n",
    "    rank().over(window_spec)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2933b3ba-ae4e-4ed2-adba-aaad0f789b5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .format(\"delta\") \\\n",
    "    .saveAsTable(\"employee_final_delta\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c434a9be-3969-4921-806c-5535bbe9ba6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM employee_final_delta\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "816768b2-b3f2-43f6-9ffa-beb96fe70178",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM catalog.schema.table;\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "spark-dataframe",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
